# PLAN: A Proof of Correctness

```
         P urposeful
         L ogistics
         A rchitecture
P  L  A  N etwork
```

### What is this?

In computer science, a "proof of [correctness](https://en.wikipedia.org/wiki/Correctness_(computer_science))" refers to a formal walk-though and demonstration that a proposed method and/or design rigorously satisfies a given set of specifications or claims.  The intention is to remove _all_ doubt that there exists a set of conditions such that the proposed method would _not_ meet all the specifications.

Below, we first express the scenario, a set of specifications, and a digital infrastructure schema.  We then proceed to demonstrate correctness for each specification, citing how the design and its operation satisfies that specification.  

Please note that some of the data structures listed below are intended to convey understanding and correctness more than they are intended to be fully performant or efficient.  You can find that step manifested as [go-plan](https://github.com/plan-tools/go-plan)

---

### Scenario

A founding set of community organizers ("admins") wish to form **C**, a secure distributed storage network comprised of computers with varying capabilities, each running a common software daemon ("node"). On their nodes, the members of **C** agree to employ **L<sub>C</sub>**, an _append-only_ [CRDT](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type) implementation whose raw data transactions are considered to be "in the clear" to adversaries (i.e. "wire" privacy is not assumed).  **C** is characterized by a set of individual members for any given point in time, with one or more members charged with administering member status, member permissions, and infrastructure oversight.  

---

### Specifications & Requirements

The members of **C** wish to assert that:
   1. _Only_ members of **C** have append access to **L<sub>C</sub>**.
   2. For all actors _not_ in **C**, all data sent to, read from, and residing on **L<sub>C</sub>** is informationally completely opaque.
   3. New members can be added to **C** at any time (given that **C** policies and permissions are met).
   4. There is a hierarchy of member admin policies and permissions that asserts itself in order to arrive at successive states (and cannot be circumvented).
   5. Assume a minority number of non-admin members are or become covert adversaries of **C**.  Even if working in concert, it must be impossible for them to: impersonate other members, insert unauthorized permissions or privileges changes, gain access to others' private keys or information, or alter **L<sub>C</sub>** in any way that poisons or destroys community content.
   6. Member admins can "delist" members from **C** such that they become equivalent to an actor that has never been a member of **C** (aside that delisted members can retain their copies of **R** before the community entered this new security "epoch").
   7. For each node **i** in **C**, it's local replica state ("**R<sub>i</sub>**"), converges to a stable/monotonic state as **L<sub>C</sub>** message traffic "catches up", for any set of network traffic delivery conditions (natural or adversarial).  That is, **R<sub>1</sub>**...**R<sub>n</sub>** update such that strong eventual consistency (SEC) is guaranteed.  
   8. If/When it is discovered that a member's personal or community keys are known to be either comprised or lost, an admin (or members previously designated by the afflicted member) initiate a new security epoch such that:
       - an adversary in possession of said keys will have no further access to **C**
       - the afflicted member's resulting security state is unaffected
   9. **C**, led by a coordinated admin effort, can switch CRDT technologies over its lifetime (e.g. **C** uses a CRDT that halts on in limiting network conditions in order to preserve security/safety, but earlier in its history when it was smaller, **C** used one that favored "liveness" over safety).


---

### Proposal

The members of **C** propose the following infrastructure:
   1. Let `UUID` represent a constant-length independently unique ID that ensures no reasonable chance of collision (typically 20 to 32 pseudo-randomly generated bytes).
   2. Each member of **C** securely maintains two "keyrings":
      1. **[]K<sub>personal</sub>**, the member's _personal keyring_, used to:
           - decrypt/encrypt information "sent" to/from that member
           - create signatures that authenticate information claimed to be authored by that member
      2. **[]K<sub>C</sub>**, the _community keyring_, used to encrypt/decrypt "community public" data (i.e. the cryptographic bridge between **L<sub>C</sub>** and **R<sub>L</sub>**)
   3. Each transaction residing on **L<sub>C</sub>** is a serialization of:
   ```
   type EntryCrypt struct {
       CommunityKeyID    UUID     // Identifies the community key used to encrypt .HeaderCrypt
       HeaderCrypt       []byte   // := Encrypt(<EntryHeader>.Marshal(), <EntryCrypt>.CommunityKeyID)
       ContentCrypt      []byte   // := Encrypt(<Body>.Marshal(), <EntryHeader>.ContentKeyID)
       Sig               []byte   // := MakeSig(<EntryCrypt>.Marshal(), KeyFor(<EntryHeader>.AuthorMemberID,
                                  //                                           <EntryHeader>.AuthorMemberEpoch))
   }
   ```
   4. Each `EntryCrypt.HeaderCrypt` is encrypted using **[]K<sub>C</sub>** and specifies a persistent `ChannelID` that it operates on within **C**'s _virtual_ channel space:
   ```
   type EntryHeader struct {
       EntryOp           int32    // Op code specifying how to interpret this entry. Typically, POST_CONTENT
       TimeSealed        int64    // Unix timestamp of when this header was encrypted and signed ("sealed")
       ChannelID         UUID     // Channel that this entry is posted to (or operates on)
       ChannelEpochID    UUID     // Epoch of the channel in effect when this entry was sealed
       AuthorMemberID    UUID     // Creator of this entry (and signer of EntryCrypt.Sig)
       AuthorMemberEpoch UUID     // Epoch of the author's identity when this entry was sealed
       ContentKeyID      UUID     // Identifies *any* key used to encrypt EntryCrypt.ContentCrypt
   }
   ```
   5. On each community node, newly arriving transactions from **L<sub>C</sub>** are decrypted using **[]K<sub>C</sub>**, verified, and deterministically merged into the node's local community "repo", **R<sub>i</sub>**:
   ```
   // A node's community replica/repo/Ri
   type CommunityRepo struct {
       Channels          map[ChannelID]ChannelStore
   }

   // Stores and provides rapid access to entries in a channel
   type ChannelStore struct {
       ChannelID         ChannelID
       Epochs            []ChannelEpoch  // The latest element is this channel's current epoch
       EntryTable        []Entry         // Contains EntryHeader info and points to ContentCrypt blob
   }

   // Represents a "rev" of this channel's security properties
   type ChannelEpoch struct {
       EpochInfo         EpochInfo
       ChannelProtocol   string          // If access control channel: "/chType/ACC"; else: "/chType/client/*"
       ChannelID         UUID            // Immutable; generated during channel genesis
       AccessChannelID   UUID            // This channel's owning ACC (and conforms to an ACC)
   }

   // Specifies general epoch params and info
   type EpochInfo struct {
       EpochStart        timestamp
       EpochID           UUID
       EpochIDPrev       UUID            // Implies a linked list of epochs extending from the past
       TransitionSecs    int             // Custom-epoch transition rules, etc
   }
   ```

   6. Entries that do not conform to channel properties or permissions are placed in a "holding tank" within **R<sub>i</sub>**.  The node periodically reattempts to merge these entries with the understanding that later-arriving entries may alter **R<sub>i</sub>** such that previously rejected entries now conform.  Entries that are rejected on the basis of an validation failure (e.g. signature validation failure) are dropped.  These rejections are cause for concern and could logged to discern bad actor patterns.
   7. Members of **C** maintain their copy of the community keyring, **[]K<sub>C</sub>**, where:
        1. **[]K<sub>C</sub>** encrypts/decrypts `EntryCrypt` traffic to/from **L<sub>C</sub>**
        2. A newly generated community key is distributed to **C**'s members via a persistent data channel using asymmetric encryption (the community admin that issues a new community key separately "sends" the key to each member in **C**'s member registry channel, encrypting the new key with the recipient members's latest public key, also available in the member registry channel)
   8. **C**'s "member registry channel" is defined as a log containing each member's UUID and current crypto "epoch":
```

// MemberEpoch represents a public "rev" of a community member's crypto
type MemberEpoch struct {
    EpochInfo             EpochInfo
    PubSigningKey         []byte
    PubCryptoKey          []byte
}
```
   7. **C**'s root "access control channel" (ACC) is a log containing access grants to member ID



```

type CommunityMember struct {
    CommunityID               UUID          // Assigned
    MemberID                  UUID          // Issued when member joins C
    type KeyRepo struct {
        CommunityKeyring []KeyEntry
    PersonalKeyring  []KeyEntry
    }
}
```


### Proof of Requirements & Claims




###

In this operating system



Let **St** be an append-only p2p replicating data store, where new data blobs can be appended and subsequently retrieved (via a transaction ID).  A node's particular state of **St**

Let
   , where the afflicted member's keys are regenerated (originating from a token generated from a community admin).  


8. **C**'s technology provisions for a "hard fork", where admins and members elect which fork to place themselves in.  

Let **σ<sub>C</sub>** be the average time period it takes for replicated network messages to reach 2/3 of the network's nodes.  This lets us set a reasonable upper-bound on how long permissions changes in **C** take to propagate.  If we were to wait 10 or 100 times **σ**, it would be safe to assume that any nodes able to receive a replicated message would have received it (if it was possible).  We thus express a time delay ceiling of permissions propagation as **kσ**.  Above this time, we assume there it is not beneficial to wait and hope that a newly arrived message will resolve a conflict.  We therefore must establish a deterministic set of rules to resolve all possible **CRS** conflicts.  For a network of 10,000 nodes in the internet of 2018, a reasonable value for **kσ** could be 3-12 hours.
